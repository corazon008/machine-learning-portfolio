{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.helper import find_project_root"
   ],
   "id": "78879ca6fe283a24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATASET_PATH = find_project_root() / Path(\"datasets/nlp/\")\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH / \"IMDB Dataset.csv.zip\")"
   ],
   "id": "7120b2ba6ba31569",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from nlp.data.preprocessing import TextPreprocessor",
   "id": "f05e8c6b42f1d2f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[\"sentiment\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})",
   "id": "b44729275d892dad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "\n",
    "train_size = int(0.7 * len(df))\n",
    "val_size = int(0.10 * len(df))\n",
    "test_size = len(df) - train_size - val_size\n",
    "print(train_size, val_size, test_size)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    df,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "X_train, y_train = train_dataset.dataset[\"review\"].reset_index(drop=True), train_dataset.dataset[\"sentiment\"].reset_index(drop=True)\n",
    "X_val, y_val = val_dataset.dataset[\"review\"].reset_index(drop=True), val_dataset.dataset[\"sentiment\"].reset_index(drop=True)\n",
    "X_test, y_test = test_dataset.dataset[\"review\"].reset_index(drop=True), test_dataset.dataset[\"sentiment\"].reset_index(drop=True)"
   ],
   "id": "cfe019246661fae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training",
   "id": "f574f8bed89de963"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from nlp.features.rnn_dataset import Vocabulary, RNNDataset, SequenceEncoder",
   "id": "d463e141341981d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "X_train_tokens = preprocessor.tokenize_batch(X_train)\n",
    "X_val_tokens = preprocessor.tokenize_batch(X_val)\n",
    "X_test_tokens = preprocessor.tokenize_batch(X_test)"
   ],
   "id": "e0a3cd914c96055a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vocab = Vocabulary(min_freq=5)\n",
    "vocab.build(X_train_tokens)"
   ],
   "id": "9118fcc031b9d248",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nlp.pipelines.rnn_pipeline import RNNPipeline\n",
    "from nlp.models.rnn_model import LSTMClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "encoder = SequenceEncoder(\n",
    "    vocab=vocab,\n",
    "    tokenizer=lambda x: x.lower().split(),\n",
    "    max_len=200\n",
    ")\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "pipeline = RNNPipeline(model, encoder, device=\"cpu\")\n",
    "\n",
    "pipeline.fit(X_train, y_train, epochs=3, X_val=X_val, y_val=y_val)"
   ],
   "id": "39895306029e2008",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "40 min\n",
    "\n",
    "Epoch 0 | Loss 0.6526 | Train Acc 0.7655 | Val Acc 0.7655\n",
    "Epoch 1 | Loss 0.4414 | Train Acc 0.8618 | Val Acc 0.8618\n",
    "Epoch 2 | Loss 0.3191 | Train Acc 0.9031 | Val Acc 0.9031"
   ],
   "id": "5201e2318078a160"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"Test accuracy:\", pipeline.score(X_test, y_test))",
   "id": "4065376872df4747",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_pred = pipeline.predict(X_test)",
   "id": "e36e259475606c99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(classification_report(y_test, y_pred))",
   "id": "f7228d9ae2673a11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "confusion_matrix(y_test, y_pred)",
   "id": "d7776ed929cde885",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mis_idx = np.where(y_pred != y_test)[0]\n",
    "\n",
    "len(mis_idx)"
   ],
   "id": "4495579953ac23e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in mis_idx[:10]:\n",
    "    print(\"\\n\\n----------\")\n",
    "    print(\"TEXT:\", X_test.iloc[i])\n",
    "    print(\"TRUE:\", y_test.iloc[i])\n",
    "    print(\"PRED:\", y_pred[i])"
   ],
   "id": "900dc301f269a3a2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
